{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import re\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "from uuid import uuid4\n",
    "from typing import Any\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import AutoTokenizer, pipeline, AutoModelForCausalLM\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.notebook import tqdm\n",
    "from IPython.display import display\n",
    "from transformers.tokenization_utils_base import BatchEncoding\n",
    "from transformers.models.llama.modeling_llama import LlamaForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"DEVICE:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "eedi_train_csv = \"data/train.csv\"\n",
    "eedi_test_csv = \"data/test.csv\"\n",
    "eedi_miscon_csv = \"data/misconception_mapping.csv\"\n",
    "llm_model_id = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "sbert_model_id = \"BAAI/bge-small-en-v1.5\"\n",
    "submission_csv = \"submission.csv\"\n",
    "intermediate_dir = \".intm\"\n",
    "random_seed = 20241030\n",
    "sample = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt\n",
    "prompt = \"\"\"Question: {Question}\n",
    "Incorrect Answer: {IncorrectAnswer}\n",
    "Correct Answer: {CorrectAnswer}\n",
    "Construct Name: {ConstructName}\n",
    "Subject Name: {SubjectName}\n",
    "\n",
    "Your task: Identify the misconception behind Incorrect Answer. Answer concisely and generically inside <response>$$INSERT TEXT HERE$$</response>.\n",
    "Before answering the question think step by step concisely in 1-2 sentence inside <thinking>$$INSERT TEXT HERE$$</thinking> tag and respond your final misconception inside <response>$$INSERT TEXT HERE$$</response> tag.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data prep utilities\n",
    "def apply_template(row, tokenizer):\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": prompt.format(\n",
    "                ConstructName=row[\"ConstructName\"],\n",
    "                SubjectName=row[\"SubjectName\"],\n",
    "                Question=row[\"QuestionText\"],\n",
    "                IncorrectAnswer=row[f\"CorrectAnswerText\"],\n",
    "                CorrectAnswer=row[f\"AnswerText\"],\n",
    "            ),\n",
    "        }\n",
    "    ]\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "    )\n",
    "    return text\n",
    "\n",
    "\n",
    "def get_correct_answer(row):\n",
    "    if row[\"CorrectAnswer\"] == \"A\":\n",
    "        return row[\"AnswerAText\"]\n",
    "    elif row[\"CorrectAnswer\"] == \"B\":\n",
    "        return row[\"AnswerBText\"]\n",
    "    elif row[\"CorrectAnswer\"] == \"C\":\n",
    "        return row[\"AnswerCText\"]\n",
    "    elif row[\"CorrectAnswer\"] == \"D\":\n",
    "        return row[\"AnswerDText\"]\n",
    "    return None\n",
    "\n",
    "\n",
    "def process_option(x, regex):\n",
    "    result = re.search(regex, x)\n",
    "    return str(result.group(1)) if result else \"\"\n",
    "\n",
    "\n",
    "def remove_prompt(record):\n",
    "    l = len(record[\"Prompt\"])\n",
    "    value = record[\"FullResponse\"][l:]\n",
    "    return value\n",
    "\n",
    "\n",
    "def extract_response(text):\n",
    "    subresponses = re.findall(r\"<response>(?s:.*?)</response>\", text)\n",
    "    subresponses = [x.strip().replace(\"<response>\", \"\").replace(\"</response>\", \"\") for x in subresponses]\n",
    "    return \" \".join(subresponses).strip()\n",
    "\n",
    "\n",
    "def dfpeek(title: str, df: pd.DataFrame) -> None:\n",
    "    print(\">>>>>>>>>>\", title, \">>>>>>>>>\")\n",
    "    display(df.head(1).transpose())\n",
    "    print(\"<<<<<<<<<<\", title, \"<<<<<<<<<<\", end=\"\\n\\n\")\n",
    "\n",
    "\n",
    "def dfpersist(trigger: bool, df: pd.DataFrame, int_dir: str, run_id: str, fn: str) -> None:\n",
    "    if not trigger:\n",
    "        return\n",
    "    assert run_id is not None\n",
    "    d = Path(intermediate_dir) / run_id\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "    p = d / fn\n",
    "    if p.exists():\n",
    "        raise FileExistsError(p.as_posix())\n",
    "    df.to_parquet(p, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_base_data(*, persist: bool = False, run_id: str = None) -> pd.DataFrame:\n",
    "    # read info\n",
    "    df = pd.read_csv(\n",
    "        eedi_train_csv,\n",
    "        dtype={\n",
    "            \"MisconceptionAId\": \"Int64\",\n",
    "            \"MisconceptionBId\": \"Int64\",\n",
    "            \"MisconceptionCId\": \"Int64\",\n",
    "            \"MisconceptionDId\": \"Int64\",\n",
    "        },\n",
    "    ).fillna(-1)\n",
    "    df_miscon = pd.read_csv(\n",
    "        eedi_miscon_csv,\n",
    "        dtype={\n",
    "            \"MisconceptionId\": \"Int64\",\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # store correct answer\n",
    "    df[\"CorrectAnswerText\"] = df.apply(get_correct_answer, axis=1)\n",
    "\n",
    "    # pivot out each wrong answer into its own row, currently the 3 wrong answers are within the same record\n",
    "    df_x = df.melt(\n",
    "        id_vars=[\n",
    "            \"QuestionId\",\n",
    "            \"ConstructName\",\n",
    "            \"SubjectName\",\n",
    "            \"QuestionText\",\n",
    "            \"CorrectAnswer\",\n",
    "            \"CorrectAnswerText\",\n",
    "        ],\n",
    "        value_vars=[\n",
    "            \"AnswerAText\",\n",
    "            \"AnswerBText\",\n",
    "            \"AnswerCText\",\n",
    "            \"AnswerDText\",\n",
    "        ],\n",
    "        var_name=\"Option\",\n",
    "        value_name=\"AnswerText\",\n",
    "    )\n",
    "    df_y = df.melt(\n",
    "        id_vars=[\n",
    "            \"QuestionId\",\n",
    "        ],\n",
    "        value_vars=[\n",
    "            \"MisconceptionAId\",\n",
    "            \"MisconceptionBId\",\n",
    "            \"MisconceptionCId\",\n",
    "            \"MisconceptionDId\",\n",
    "        ],\n",
    "        var_name=\"Option\",\n",
    "        value_name=\"MisconceptionId\",\n",
    "    )\n",
    "\n",
    "    # remap option values of from \"xxxxXxxxx\" to \"X\"\n",
    "    df_x[\"Option\"] = df_x[\"Option\"].map(partial(process_option, regex=r\"Answer([A-D])Text\"))\n",
    "    df_y[\"Option\"] = df_y[\"Option\"].map(partial(process_option, regex=r\"Misconception([A-D])Id\"))\n",
    "\n",
    "    # mark correct answers\n",
    "    df_x[\"IsCorrectAnswer\"] = df_x[\"CorrectAnswer\"] == df_x[\"Option\"]\n",
    "\n",
    "    # create primary key, drop components, reorder col\n",
    "    df_x[\"QuestionId_Answer\"] = df_x[\"QuestionId\"].astype(str) + \"_\" + df_x[\"Option\"].astype(str)\n",
    "    df_y[\"QuestionId_Answer\"] = df_y[\"QuestionId\"].astype(str) + \"_\" + df_y[\"Option\"].astype(str)\n",
    "    df_x.drop(columns=[\"QuestionId\", \"Option\"], inplace=True)\n",
    "    df_y.drop(columns=[\"QuestionId\", \"Option\"], inplace=True)\n",
    "    df_x = df_x[[\"QuestionId_Answer\"] + [c for c in df_x.columns if c != \"QuestionId_Answer\"]]\n",
    "    df_y = df_y[[\"QuestionId_Answer\"] + [c for c in df_y.columns if c != \"QuestionId_Answer\"]]\n",
    "\n",
    "    # map misconception text to labels\n",
    "    df_y = df_y.join(df_miscon, on=\"MisconceptionId\", how=\"left\", lsuffix=\"a\", rsuffix=\"b\")\n",
    "    df_y = df_y[[\"QuestionId_Answer\", \"MisconceptionId\", \"MisconceptionName\"]]\n",
    "\n",
    "    # merge datasets\n",
    "    df_xy = df_x.merge(df_y, how=\"left\", on=\"QuestionId_Answer\")\n",
    "\n",
    "    # persist df_xy\n",
    "    dfpersist(persist, df_xy, intermediate_dir, run_id, \"df_xy.parquet\")\n",
    "\n",
    "    return df_xy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# options to filter data\n",
    "\n",
    "def filter_by_wrong_answers_only(df_xy: pd.DataFrame) -> pd.DataFrame:\n",
    "    df_xy = df_xy[~df_xy[\"IsCorrectAnswer\"]]\n",
    "    return df_xy\n",
    "\n",
    "\n",
    "def filter_by_wrong_answers_and_misconceptions(df_xy: pd.DataFrame) -> pd.DataFrame:\n",
    "    return df_xy[(~df_xy[\"IsCorrectAnswer\"]) & (df_xy[\"MisconceptionId\"] != -1)]\n",
    "\n",
    "\n",
    "def no_filter(df_xy: pd.DataFrame) -> pd.DataFrame:\n",
    "    return df_xy\n",
    "\n",
    "\n",
    "def filter_data(df_xy: pd.DataFrame, persist: bool = False, run_id: str = None) -> pd.DataFrame:\n",
    "    # NOTE: choose filter func\n",
    "    filter_func = filter_by_wrong_answers_and_misconceptions\n",
    "\n",
    "    # filter df_xy\n",
    "    df_xy_filtered = filter_func(df_xy)\n",
    "\n",
    "    # persist df_xy_filtered\n",
    "    dfpersist(persist, df_xy_filtered, intermediate_dir, run_id, \"df_xy_filtered.parquet\")\n",
    "\n",
    "    return df_xy_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize data\n",
    "\n",
    "def tokenize_for_llm(\n",
    "    tokenizer: Any,\n",
    "    df_xy: pd.DataFrame,\n",
    "    *,\n",
    "    persist: bool = False,\n",
    "    persist_fn: str = \"df_prompt.parquet\",\n",
    "    run_id: str = None,\n",
    ") -> tuple[pd.DataFrame, BatchEncoding]:\n",
    "    # generate prompts from records\n",
    "    df_prompt = df_xy.copy(deep=True)\n",
    "    # tokenizer = AutoTokenizer.from_pretrained(llm_model_id)\n",
    "    df_prompt[\"Prompt\"] = df_prompt.apply(\n",
    "        partial(apply_template, tokenizer=tokenizer),\n",
    "        axis=1,\n",
    "    )\n",
    "    df_prompt = df_prompt[[\"QuestionId_Answer\", \"Prompt\"]]\n",
    "\n",
    "    # persist df_xy_filtered\n",
    "    dfpersist(persist, df_prompt, intermediate_dir, run_id, persist_fn)\n",
    "\n",
    "    # tokenize : NOTE configure as required\n",
    "    model_inputs = tokenizer(df_prompt[\"Prompt\"].to_list(), return_tensors=\"pt\", padding=True)\n",
    "\n",
    "    return df_prompt, model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main functions\n",
    "\n",
    "def generate_zeroshot():\n",
    "    pass\n",
    "\n",
    "\n",
    "def generate_predictions():\n",
    "    pass\n",
    "\n",
    "\n",
    "def evaluate():\n",
    "    pass\n",
    "\n",
    "\n",
    "def main():\n",
    "    run_id = str(uuid4())\n",
    "    print(\"run_id:\", run_id)\n",
    "    df_xy = prepare_base_data(persist=True, run_id=run_id)\n",
    "    df_xy = filter_data(df_xy, persist=True, run_id=run_id)\n",
    "\n",
    "    if sample:\n",
    "        df_xy = df_xy.sample(20)\n",
    "\n",
    "    df_xy_train, df_xy_test = train_test_split(df_xy, test_size=0.2, random_state=random_seed)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(llm_model_id)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    df_prompt_train, tokens_train = tokenize_for_llm(tokenizer, df_xy_train, persist=True, persist_fn=\"df_prompt_train.parquet\", run_id=run_id,)\n",
    "    df_prompt_test, tokens_test = tokenize_for_llm(tokenizer, df_xy_test, persist=True, persist_fn=\"df_prompt_test.parquet\", run_id=run_id,)\n",
    "    # model: LlamaForCausalLM = AutoModelForCausalLM.from_pretrained(llm_model_id)\n",
    "    # output = model(**tokens_train)\n",
    "    # tokenizer(output)\n",
    "    return tokenizer, tokens_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# entrypoint\n",
    "tokenizer, tokens_train = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_train['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(tokens_train[\"input_ids\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model: LlamaForCausalLM = AutoModelForCausalLM.from_pretrained(llm_model_id).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_train = tokens_train.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(**tokens_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = output.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_token_id = torch.argmax(logits[:, -1, :], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_token_id.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_text = tokenizer.decode(predicted_token_id, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eedi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
