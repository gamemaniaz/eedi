{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gamem\\mambaforge\\envs\\eedi\\lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import re\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "from uuid import uuid4\n",
    "from typing import Any\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import AutoTokenizer, pipeline, AutoModelForCausalLM\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.notebook import tqdm\n",
    "from IPython.display import display\n",
    "from transformers.tokenization_utils_base import BatchEncoding\n",
    "from transformers.models.llama.modeling_llama import LlamaForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE: cuda\n"
     ]
    }
   ],
   "source": [
    "# env\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"DEVICE:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "eedi_train_csv = \"data/train.csv\"\n",
    "eedi_test_csv = \"data/test.csv\"\n",
    "eedi_miscon_csv = \"data/misconception_mapping.csv\"\n",
    "llm_model_id = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "sbert_model_id = \"BAAI/bge-small-en-v1.5\"\n",
    "submission_csv = \"submission.csv\"\n",
    "intermediate_dir = \".intm\"\n",
    "random_seed = 20241030\n",
    "sample = True\n",
    "sample_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt\n",
    "prompt = \"\"\"Question: {Question}\n",
    "Incorrect Answer: {IncorrectAnswer}\n",
    "Correct Answer: {CorrectAnswer}\n",
    "Construct Name: {ConstructName}\n",
    "Subject Name: {SubjectName}\n",
    "\n",
    "Your task: Identify the misconception behind Incorrect Answer. Answer concisely and generically inside <response>$$INSERT TEXT HERE$$</response>.\n",
    "Before answering the question think step by step concisely in 1-2 sentence inside <thinking>$$INSERT TEXT HERE$$</thinking> tag and respond your final misconception inside <response>$$INSERT TEXT HERE$$</response> tag.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data prep utilities\n",
    "def apply_template(row, tokenizer):\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": prompt.format(\n",
    "                ConstructName=row[\"ConstructName\"],\n",
    "                SubjectName=row[\"SubjectName\"],\n",
    "                Question=row[\"QuestionText\"],\n",
    "                IncorrectAnswer=row[f\"CorrectAnswerText\"],\n",
    "                CorrectAnswer=row[f\"AnswerText\"],\n",
    "            ),\n",
    "        }\n",
    "    ]\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "    )\n",
    "    return text\n",
    "\n",
    "\n",
    "def get_correct_answer(row):\n",
    "    if row[\"CorrectAnswer\"] == \"A\":\n",
    "        return row[\"AnswerAText\"]\n",
    "    elif row[\"CorrectAnswer\"] == \"B\":\n",
    "        return row[\"AnswerBText\"]\n",
    "    elif row[\"CorrectAnswer\"] == \"C\":\n",
    "        return row[\"AnswerCText\"]\n",
    "    elif row[\"CorrectAnswer\"] == \"D\":\n",
    "        return row[\"AnswerDText\"]\n",
    "    return None\n",
    "\n",
    "\n",
    "def process_option(x, regex):\n",
    "    result = re.search(regex, x)\n",
    "    return str(result.group(1)) if result else \"\"\n",
    "\n",
    "\n",
    "def remove_prompt(record):\n",
    "    l = len(record[\"Prompt\"])\n",
    "    value = record[\"FullResponse\"][l:]\n",
    "    return value\n",
    "\n",
    "\n",
    "def extract_response(text):\n",
    "    subresponses = re.findall(r\"<response>(?s:.*?)</response>\", text)\n",
    "    subresponses = [x.strip().replace(\"<response>\", \"\").replace(\"</response>\", \"\") for x in subresponses]\n",
    "    return \" \".join(subresponses).strip()\n",
    "\n",
    "\n",
    "def dfpeek(title: str, df: pd.DataFrame) -> None:\n",
    "    print(\">>>>>>>>>>\", title, \">>>>>>>>>\")\n",
    "    display(df.head(1).transpose())\n",
    "    print(\"<<<<<<<<<<\", title, \"<<<<<<<<<<\", end=\"\\n\\n\")\n",
    "\n",
    "\n",
    "def dfpersist(trigger: bool, df: pd.DataFrame, int_dir: str, run_id: str, fn: str) -> None:\n",
    "    if not trigger:\n",
    "        return\n",
    "    assert run_id is not None\n",
    "    d = Path(intermediate_dir) / run_id\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "    p = d / fn\n",
    "    if p.exists():\n",
    "        raise FileExistsError(p.as_posix())\n",
    "    df.to_parquet(p, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_base_data(*, persist: bool = False, run_id: str = None) -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    # read info\n",
    "    df = pd.read_csv(\n",
    "        eedi_train_csv,\n",
    "        dtype={\n",
    "            \"MisconceptionAId\": \"Int64\",\n",
    "            \"MisconceptionBId\": \"Int64\",\n",
    "            \"MisconceptionCId\": \"Int64\",\n",
    "            \"MisconceptionDId\": \"Int64\",\n",
    "        },\n",
    "    ).fillna(-1)\n",
    "    df_miscon = pd.read_csv(\n",
    "        eedi_miscon_csv,\n",
    "        dtype={\n",
    "            \"MisconceptionId\": \"Int64\",\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # store correct answer\n",
    "    df[\"CorrectAnswerText\"] = df.apply(get_correct_answer, axis=1)\n",
    "\n",
    "    # pivot out each wrong answer into its own row, currently the 3 wrong answers are within the same record\n",
    "    df_x = df.melt(\n",
    "        id_vars=[\n",
    "            \"QuestionId\",\n",
    "            \"ConstructName\",\n",
    "            \"SubjectName\",\n",
    "            \"QuestionText\",\n",
    "            \"CorrectAnswer\",\n",
    "            \"CorrectAnswerText\",\n",
    "        ],\n",
    "        value_vars=[\n",
    "            \"AnswerAText\",\n",
    "            \"AnswerBText\",\n",
    "            \"AnswerCText\",\n",
    "            \"AnswerDText\",\n",
    "        ],\n",
    "        var_name=\"Option\",\n",
    "        value_name=\"AnswerText\",\n",
    "    )\n",
    "    df_y = df.melt(\n",
    "        id_vars=[\n",
    "            \"QuestionId\",\n",
    "        ],\n",
    "        value_vars=[\n",
    "            \"MisconceptionAId\",\n",
    "            \"MisconceptionBId\",\n",
    "            \"MisconceptionCId\",\n",
    "            \"MisconceptionDId\",\n",
    "        ],\n",
    "        var_name=\"Option\",\n",
    "        value_name=\"MisconceptionId\",\n",
    "    )\n",
    "\n",
    "    # remap option values of from \"xxxxXxxxx\" to \"X\"\n",
    "    df_x[\"Option\"] = df_x[\"Option\"].map(partial(process_option, regex=r\"Answer([A-D])Text\"))\n",
    "    df_y[\"Option\"] = df_y[\"Option\"].map(partial(process_option, regex=r\"Misconception([A-D])Id\"))\n",
    "\n",
    "    # mark correct answers\n",
    "    df_x[\"IsCorrectAnswer\"] = df_x[\"CorrectAnswer\"] == df_x[\"Option\"]\n",
    "\n",
    "    # create primary key, drop components, reorder col\n",
    "    df_x[\"QuestionId_Answer\"] = df_x[\"QuestionId\"].astype(str) + \"_\" + df_x[\"Option\"].astype(str)\n",
    "    df_y[\"QuestionId_Answer\"] = df_y[\"QuestionId\"].astype(str) + \"_\" + df_y[\"Option\"].astype(str)\n",
    "    df_x.drop(columns=[\"QuestionId\", \"Option\"], inplace=True)\n",
    "    df_y.drop(columns=[\"QuestionId\", \"Option\"], inplace=True)\n",
    "    df_x = df_x[[\"QuestionId_Answer\"] + [c for c in df_x.columns if c != \"QuestionId_Answer\"]]\n",
    "    df_y = df_y[[\"QuestionId_Answer\"] + [c for c in df_y.columns if c != \"QuestionId_Answer\"]]\n",
    "\n",
    "    # map misconception text to labels\n",
    "    df_y = df_y.join(df_miscon, on=\"MisconceptionId\", how=\"left\", lsuffix=\"a\", rsuffix=\"b\")\n",
    "    df_y = df_y[[\"QuestionId_Answer\", \"MisconceptionId\", \"MisconceptionName\"]]\n",
    "\n",
    "    # merge datasets\n",
    "    df_xy = df_x.merge(df_y, how=\"left\", on=\"QuestionId_Answer\")\n",
    "\n",
    "    # persist df_xy\n",
    "    dfpersist(persist, df_xy, intermediate_dir, run_id, \"df_xy.parquet\")\n",
    "\n",
    "    return df_xy, df_miscon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# options to filter data\n",
    "\n",
    "def filter_by_wrong_answers_only(df_xy: pd.DataFrame) -> pd.DataFrame:\n",
    "    df_xy = df_xy[~df_xy[\"IsCorrectAnswer\"]]\n",
    "    return df_xy\n",
    "\n",
    "\n",
    "def filter_by_wrong_answers_and_misconceptions(df_xy: pd.DataFrame) -> pd.DataFrame:\n",
    "    return df_xy[(~df_xy[\"IsCorrectAnswer\"]) & (df_xy[\"MisconceptionId\"] != -1)]\n",
    "\n",
    "\n",
    "def no_filter(df_xy: pd.DataFrame) -> pd.DataFrame:\n",
    "    return df_xy\n",
    "\n",
    "\n",
    "def filter_data(df_xy: pd.DataFrame, persist: bool = False, run_id: str = None) -> pd.DataFrame:\n",
    "    filter_func = filter_by_wrong_answers_and_misconceptions\n",
    "    df_xy_filtered = filter_func(df_xy)\n",
    "    dfpersist(persist, df_xy_filtered, intermediate_dir, run_id, \"df_xy_filtered.parquet\")\n",
    "    return df_xy_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize data\n",
    "\n",
    "def tokenize_for_llm(\n",
    "    tokenizer: Any,\n",
    "    df_xy: pd.DataFrame,\n",
    "    *,\n",
    "    persist: bool = False,\n",
    "    persist_fn: str = \"df_prompt.parquet\",\n",
    "    run_id: str = None,\n",
    ") -> tuple[pd.DataFrame, BatchEncoding]:\n",
    "    df_prompt = df_xy.copy(deep=True)\n",
    "    df_prompt[\"Prompt\"] = df_prompt.apply(\n",
    "        partial(apply_template, tokenizer=tokenizer),\n",
    "        axis=1,\n",
    "    )\n",
    "    df_prompt = df_prompt[[\"QuestionId_Answer\", \"Prompt\"]]\n",
    "    dfpersist(persist, df_prompt, intermediate_dir, run_id, persist_fn)\n",
    "    model_inputs = tokenizer(df_prompt[\"Prompt\"].to_list(), return_tensors=\"pt\", padding=True).to(device)\n",
    "    return df_prompt, model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_zeroshot(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    tokens,\n",
    "    df_prompt,\n",
    "    *,\n",
    "    persist: bool = False,\n",
    "    persist_fn: str = \"df_responses.parquet\",\n",
    "    run_id: str = None,\n",
    ") -> pd.DataFrame:\n",
    "    output_ids = model.generate(tokens.input_ids, max_new_tokens=4096, num_return_sequences=1)\n",
    "    responses = tokenizer.batch_decode(output_ids, skip_special_tokens=True) # list[str]\n",
    "    df_prompt[\"FullResponse\"] = responses\n",
    "    df_prompt[\"Misconception\"] = [extract_response(x) for x in df_prompt[\"FullResponse\"]]\n",
    "    dfpersist(persist, df_prompt, intermediate_dir, run_id, persist_fn)\n",
    "    return df_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_misconceptions(\n",
    "    model,\n",
    "    df_responses,\n",
    "    df_miscon,\n",
    "    *,\n",
    "    persist: bool = False,\n",
    "    persist_fn: str = \"df_submission.parquet\",\n",
    "    run_id: str = None,\n",
    "):\n",
    "    print(\">> generate_misconceptions\")\n",
    "    embedding_query = model.encode(df_responses[\"Misconception\"].values)\n",
    "    embedding_miscon = model.encode(df_miscon[\"MisconceptionName\"].values)\n",
    "    cosine_similarities = cosine_similarity(embedding_query, embedding_miscon)\n",
    "    rev_sorted_indices = np.argsort(-cosine_similarities, axis=1)\n",
    "    df_responses[\"MisconceptionId\"] = rev_sorted_indices[:, :25].tolist()\n",
    "    df_responses[\"MisconceptionId\"] = df_responses[\"MisconceptionId\"].apply(lambda x: \" \".join(map(str, x)))\n",
    "    df_submission = df_responses[[\"QuestionId_Answer\", \"MisconceptionId\"]]\n",
    "    dfpersist(persist, df_submission, intermediate_dir, run_id, persist_fn)\n",
    "    return df_submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apk(actual, predicted, k=25):\n",
    "    if not actual:\n",
    "        return 0.0\n",
    "\n",
    "    actual = [actual]\n",
    "    predicted = list(map(int, predicted.split()))\n",
    "\n",
    "    if len(predicted) > k:\n",
    "        predicted = predicted[:k]\n",
    "\n",
    "    score = 0.0\n",
    "    num_hits = 0.0\n",
    "\n",
    "    for i,p in enumerate(predicted):\n",
    "        # first condition checks whether it is valid prediction\n",
    "        # second condition checks if prediction is not repeated\n",
    "        if p in actual and p not in predicted[:i]:\n",
    "            num_hits += 1.0\n",
    "            score += num_hits / (i+1.0)\n",
    "\n",
    "    return score / min(len(actual), k)\n",
    "\n",
    "def mapk(actual, predicted, k=25):\n",
    "    return np.mean([apk(a, p, k) for a, p in zip(actual, predicted)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(df_xy, df_submission, *, run_id: str, fn: str = \"results.txt\"):\n",
    "    results = mapk(df_xy[\"MisconceptionId\"].to_list(), df_submission[\"MisconceptionId\"].to_list())\n",
    "    assert run_id is not None\n",
    "    d = Path(intermediate_dir) / run_id\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "    p: Path = d / fn\n",
    "    if p.exists():\n",
    "        raise FileExistsError(p.as_posix())\n",
    "    p.write_text(f\"Results of MapK=25 : {results}\")\n",
    "    print(\"Results of MapK=25 :\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main functions\n",
    "\n",
    "def main():\n",
    "    run_id = str(uuid4())\n",
    "    print(\"run_id:\", run_id)\n",
    "    df_xy, df_miscon = prepare_base_data(persist=True, run_id=run_id)\n",
    "    df_xy = filter_data(df_xy, persist=True, run_id=run_id)\n",
    "\n",
    "    if sample:\n",
    "        df_xy = df_xy.sample(sample_size)\n",
    "\n",
    "    # df_xy_train, df_xy_test = train_test_split(df_xy, test_size=0.2, random_state=random_seed)\n",
    "    # tokenizer = AutoTokenizer.from_pretrained(llm_model_id)\n",
    "    # tokenizer.pad_token = tokenizer.eos_token\n",
    "    # df_prompt_train, tokens_train = tokenize_for_llm(tokenizer, df_xy_train, persist=True, persist_fn=\"df_prompt_train.parquet\", run_id=run_id,)\n",
    "    # df_prompt_test, tokens_test = tokenize_for_llm(tokenizer, df_xy_test, persist=True, persist_fn=\"df_prompt_test.parquet\", run_id=run_id,)\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(llm_model_id)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    model: LlamaForCausalLM = AutoModelForCausalLM.from_pretrained(llm_model_id).to(device)\n",
    "    sbert_model = SentenceTransformer(sbert_model_id)\n",
    "\n",
    "    df_prompt, tokens = tokenize_for_llm(tokenizer, df_xy, persist=True, run_id=run_id)\n",
    "    df_responses = generate_zeroshot(model, tokenizer, tokens, df_prompt, persist=True, run_id=run_id)\n",
    "    df_submission = generate_misconceptions(sbert_model, df_responses, df_miscon, persist=True, run_id=run_id)\n",
    "\n",
    "    evaluate(df_xy, df_submission, run_id=run_id)\n",
    "\n",
    "    return df_xy, df_submission\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run_id: 537d5b29-03ad-4aa4-8db1-d1ca74dd43da\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> generate_misconceptions\n",
      "Results of MapK=25 : 0.0\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eedi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
