{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt Engineering\n",
    "# https://isamu-website.medium.com/understanding-the-current-state-of-reasoning-with-llms-dbd9fa3fc1a0\n",
    "# https://www.promptingguide.ai/\n",
    "\n",
    "# Techniques to try\n",
    "# Chain of Thought\n",
    "# Meta Prompting\n",
    "# Tree of Thoughts\n",
    "# Buffer of Thoughts\n",
    "# Generated Knowledge Prompting\n",
    "# Selective CoT\n",
    "# Intermediate Step Evaluation\n",
    "\n",
    "# If you are more adventurous, you can try knowledge graph related prompting.\n",
    "\n",
    "# Tasks\n",
    "# Data and Workflow Engineering\n",
    "# EDA\n",
    "# Tokenizing\n",
    "# Data Cleaning\n",
    "# Prediction\n",
    "# Submission\n",
    "# Model Review & Selection\n",
    "# Feature Engineering\n",
    "# CLIP\n",
    "# Prompt Engineering\n",
    "# Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "TASKS\n",
    "\n",
    "basic tasks\n",
    "- data ingestion\n",
    "- data cleaning\n",
    "- data wrapping\n",
    "- tokenization\n",
    "- prediction\n",
    "- submission\n",
    "\n",
    "advanced tasks\n",
    "- fine tuning\n",
    "- prompt engineering\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    pipeline,\n",
    "    BitsAndBytesConfig,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\n",
    "    \"data/train.csv\",\n",
    "    dtype={\n",
    "        \"MisconceptionAId\": \"Int64\",\n",
    "        \"MisconceptionBId\": \"Int64\",\n",
    "        \"MisconceptionCId\": \"Int64\",\n",
    "        \"MisconceptionDId\": \"Int64\",\n",
    "    },\n",
    ").fillna(-1)\n",
    "df_test = pd.read_csv(\"data/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_train.head(1).transpose())\n",
    "print(df_train.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_test.head(1).transpose())\n",
    "print(df_test.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_id = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "model_id = \"unsloth/llama-3-8b-bnb-4bit\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT = \"\"\"Question: {Question}\n",
    "Incorrect Answer: {IncorrectAnswer}\n",
    "Correct Answer: {CorrectAnswer}\n",
    "Construct Name: {ConstructName}\n",
    "Subject Name: {SubjectName}\n",
    "\n",
    "Your task: Identify the misconception behind Incorrect Answer. Answer concisely and generically inside <response>$$INSERT TEXT HERE$$</response>.\n",
    "Before answering the question think step by step concisely in 1-2 sentence inside <thinking>$$INSERT TEXT HERE$$</thinking> tag and respond your final misconception inside <response>$$INSERT TEXT HERE$$</response> tag.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_template(row, tokenizer):\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": PROMPT.format(\n",
    "                ConstructName=row[\"ConstructName\"],\n",
    "                SubjectName=row[\"SubjectName\"],\n",
    "                Question=row[\"QuestionText\"],\n",
    "                IncorrectAnswer=row[f\"CorrectAnswerText\"],\n",
    "                CorrectAnswer=row[f\"AnswerText\"],\n",
    "            ),\n",
    "        }\n",
    "    ]\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_correct_answer(row):\n",
    "    if row[\"CorrectAnswer\"] == \"A\":\n",
    "        return row[\"AnswerAText\"]\n",
    "    elif row[\"CorrectAnswer\"] == \"B\":\n",
    "        return row[\"AnswerBText\"]\n",
    "    elif row[\"CorrectAnswer\"] == \"C\":\n",
    "        return row[\"AnswerCText\"]\n",
    "    elif row[\"CorrectAnswer\"] == \"D\":\n",
    "        return row[\"AnswerDText\"]\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test[\"CorrectAnswerText\"] = df_test.apply(get_correct_answer, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_column = [\n",
    "    \"QuestionId\",\n",
    "    \"ConstructName\",\n",
    "    \"SubjectName\",\n",
    "    \"CorrectAnswer\",\n",
    "    \"QuestionText\",\n",
    "    \"CorrectAnswerText\",\n",
    "]\n",
    "df_answer = pd.melt(\n",
    "    df_test,\n",
    "    id_vars=select_column,\n",
    "    value_vars=[f\"Answer{ans}Text\" for ans in [\"A\", \"B\", \"C\", \"D\"]],\n",
    "    var_name=\"Option\",\n",
    "    value_name=\"AnswerText\",\n",
    ").sort_values(\"QuestionId\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_answer.head(1).transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_option(x):\n",
    "    out = re.search(r\"Answer([A-D])\", x)\n",
    "    if out:\n",
    "        return out.group(1)\n",
    "    return None\n",
    "\n",
    "\n",
    "df_answer[\"Option\"] = df_answer[\"Option\"].apply(process_option)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "# tokenizer.chat_template = (\n",
    "#     \"{{bos_token}}\"\n",
    "#     \"{% for message in messages %}\"\n",
    "#         \"<|start_header_id|>{{message['role']}}<|end_header_id|>\\n\\n{{message['content']}}<|eot_id|>\"\n",
    "#     \"{% endfor %}\"\n",
    "#     \"{% if add_generation_prompt%}<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "#         \"{% else %}{{eos_token}}\"\n",
    "#     \"{% endif %}\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_answer = df_answer[df_answer[\"CorrectAnswer\"] != df_answer[\"Option\"]]\n",
    "df_answer[\"Prompt\"] = df_answer.apply(\n",
    "    lambda row: apply_template(row, tokenizer), axis=1\n",
    ")\n",
    "df_answer.to_parquet(\"test.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"test.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nf4_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.float16,\n",
    "    low_cpu_mem_usage=True,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=nf4_config,\n",
    ")\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "# pipeline = pipeline(\"text-generation\", model=model_id, model_kwargs={\"load_in_4bit\": True}, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.metadata.version(\"bitsandbytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses = []\n",
    "for v in tqdm(df[\"Prompt\"].values):\n",
    "    responses.append(pipeline(v, max_new_tokens=512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses = [x[0][\"generated_text\"] for x in responses]\n",
    "df[\"FullResponse\"] = responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_response(text):\n",
    "    return \",\".join(re.findall(r\"<response>(.*?)</response>\", text)).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses = [extract_response(x) for x in responses]\n",
    "df[\"Misconception\"] = responses\n",
    "df.to_parquet(\"output.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"output.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[\"FullResponse\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(1).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eedi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
