{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-10-28T08:28:38.184213Z","iopub.status.busy":"2024-10-28T08:28:38.183372Z","iopub.status.idle":"2024-10-28T08:28:38.191307Z","shell.execute_reply":"2024-10-28T08:28:38.190315Z","shell.execute_reply.started":"2024-10-28T08:28:38.184159Z"},"trusted":true},"outputs":[],"source":["# # This Python 3 environment comes with many helpful analytics libraries installed\n","# # It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# # For example, here's several helpful packages to load\n","\n","# import numpy as np # linear algebra\n","# import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# # Input data files are available in the read-only \"../input/\" directory\n","# # For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","# import os\n","# for dirname, _, filenames in os.walk('/kaggle/input'):\n","#     for filename in filenames:\n","#         print(os.path.join(dirname, filename))\n","\n","# # You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n","# # You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]},{"cell_type":"markdown","metadata":{},"source":["### Imports"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-10-28T08:28:38.196977Z","iopub.status.busy":"2024-10-28T08:28:38.196383Z","iopub.status.idle":"2024-10-28T08:28:45.618925Z","shell.execute_reply":"2024-10-28T08:28:45.618112Z","shell.execute_reply.started":"2024-10-28T08:28:38.196931Z"},"trusted":true},"outputs":[],"source":["import re\n","from pathlib import Path\n","\n","import pandas as pd\n","import torch\n","from transformers import (\n","    AutoModelForCausalLM,\n","    AutoTokenizer,\n","    BitsAndBytesConfig,\n","    pipeline,\n",")\n","from transformers.models.llama.modeling_llama import LlamaForCausalLM\n","from tqdm import tqdm"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-10-28T08:28:45.620851Z","iopub.status.busy":"2024-10-28T08:28:45.620151Z","iopub.status.idle":"2024-10-28T08:28:45.625428Z","shell.execute_reply":"2024-10-28T08:28:45.624375Z","shell.execute_reply.started":"2024-10-28T08:28:45.620811Z"},"trusted":true},"outputs":[],"source":["# PATHS\n","train_csv = Path(\"data/train.csv\")\n","test_csv = Path(\"data/test.csv\")"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-10-28T08:28:45.628417Z","iopub.status.busy":"2024-10-28T08:28:45.628118Z","iopub.status.idle":"2024-10-28T08:28:45.637934Z","shell.execute_reply":"2024-10-28T08:28:45.636879Z","shell.execute_reply.started":"2024-10-28T08:28:45.628384Z"},"trusted":true},"outputs":[],"source":["model_id = \"meta-llama/Llama-3.2-3B-Instruct\""]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-10-28T08:28:45.639351Z","iopub.status.busy":"2024-10-28T08:28:45.639034Z","iopub.status.idle":"2024-10-28T08:28:45.688584Z","shell.execute_reply":"2024-10-28T08:28:45.687499Z","shell.execute_reply.started":"2024-10-28T08:28:45.639318Z"},"trusted":true},"outputs":[],"source":["df_train = pd.read_csv(\n","    train_csv,\n","    dtype={\n","        \"MisconceptionAId\": \"Int64\",\n","        \"MisconceptionBId\": \"Int64\",\n","        \"MisconceptionCId\": \"Int64\",\n","        \"MisconceptionDId\": \"Int64\",\n","    },\n",").fillna(-1)\n","df_test = pd.read_csv(test_csv)"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-10-28T08:28:45.690657Z","iopub.status.busy":"2024-10-28T08:28:45.690299Z","iopub.status.idle":"2024-10-28T08:28:45.695523Z","shell.execute_reply":"2024-10-28T08:28:45.694456Z","shell.execute_reply.started":"2024-10-28T08:28:45.690619Z"},"trusted":true},"outputs":[],"source":["PROMPT  = \"\"\"Question: {Question}\n","Incorrect Answer: {IncorrectAnswer}\n","Correct Answer: {CorrectAnswer}\n","Construct Name: {ConstructName}\n","Subject Name: {SubjectName}\n","\n","Your task: Identify the misconception behind Incorrect Answer. Answer concisely and generically inside <response>$$INSERT TEXT HERE$$</response>.\n","Before answering the question think step by step concisely in 1-2 sentence inside <thinking>$$INSERT TEXT HERE$$</thinking> tag and respond your final misconception inside <response>$$INSERT TEXT HERE$$</response> tag.\"\"\""]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-10-28T08:28:45.697261Z","iopub.status.busy":"2024-10-28T08:28:45.696911Z","iopub.status.idle":"2024-10-28T08:28:45.706761Z","shell.execute_reply":"2024-10-28T08:28:45.705897Z","shell.execute_reply.started":"2024-10-28T08:28:45.697225Z"},"trusted":true},"outputs":[],"source":["def apply_template(row, tokenizer):\n","    messages = [\n","        {\n","            \"role\": \"user\",\n","            \"content\": PROMPT.format(\n","                ConstructName=row[\"ConstructName\"],\n","                SubjectName=row[\"SubjectName\"],\n","                Question=row[\"QuestionText\"],\n","                IncorrectAnswer=row[f\"CorrectAnswerText\"],\n","                CorrectAnswer=row[f\"AnswerText\"],\n","            ),\n","        }\n","    ]\n","    text = tokenizer.apply_chat_template(\n","        messages, tokenize=False, add_generation_prompt=True\n","    )\n","    return text"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-10-28T08:28:45.708903Z","iopub.status.busy":"2024-10-28T08:28:45.707833Z","iopub.status.idle":"2024-10-28T08:28:45.717468Z","shell.execute_reply":"2024-10-28T08:28:45.716548Z","shell.execute_reply.started":"2024-10-28T08:28:45.708851Z"},"trusted":true},"outputs":[],"source":["def get_correct_answer(row):\n","    if row[\"CorrectAnswer\"] == \"A\":\n","        return row[\"AnswerAText\"]\n","    elif row[\"CorrectAnswer\"] == \"B\":\n","        return row[\"AnswerBText\"]\n","    elif row[\"CorrectAnswer\"] == \"C\":\n","        return row[\"AnswerCText\"]\n","    elif row[\"CorrectAnswer\"] == \"D\":\n","        return row[\"AnswerDText\"]\n","    else:\n","        return None"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-10-28T08:28:45.719358Z","iopub.status.busy":"2024-10-28T08:28:45.718790Z","iopub.status.idle":"2024-10-28T08:28:45.738740Z","shell.execute_reply":"2024-10-28T08:28:45.737802Z","shell.execute_reply.started":"2024-10-28T08:28:45.719320Z"},"trusted":true},"outputs":[],"source":["df_test[\"CorrectAnswerText\"] = df_test.apply(get_correct_answer, axis=1)\n","select_column = [\n","    \"QuestionId\",\n","    \"ConstructName\",\n","    \"SubjectName\",\n","    \"CorrectAnswer\",\n","    \"QuestionText\",\n","    \"CorrectAnswerText\",\n","]\n","df_answer = pd.melt(\n","    df_test,\n","    id_vars=select_column,\n","    value_vars=[f\"Answer{ans}Text\" for ans in [\"A\", \"B\", \"C\", \"D\"]],\n","    var_name=\"Option\",\n","    value_name=\"AnswerText\",\n",").sort_values(\"QuestionId\")"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-10-28T08:28:45.740314Z","iopub.status.busy":"2024-10-28T08:28:45.739975Z","iopub.status.idle":"2024-10-28T08:28:45.749912Z","shell.execute_reply":"2024-10-28T08:28:45.748949Z","shell.execute_reply.started":"2024-10-28T08:28:45.740279Z"},"trusted":true},"outputs":[],"source":["def process_option(x):\n","    out = re.search(r\"Answer([A-D])\", x)\n","    if out:\n","        return str(out.group(1))\n","    return \"\"\n","\n","df_answer[\"Option\"] = df_answer[\"Option\"].map(process_option)"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-10-28T08:28:45.751584Z","iopub.status.busy":"2024-10-28T08:28:45.751182Z","iopub.status.idle":"2024-10-28T08:28:46.302887Z","shell.execute_reply":"2024-10-28T08:28:46.302109Z","shell.execute_reply.started":"2024-10-28T08:28:45.751537Z"},"trusted":true},"outputs":[],"source":["tokenizer = AutoTokenizer.from_pretrained(model_id)"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-10-28T08:28:46.304325Z","iopub.status.busy":"2024-10-28T08:28:46.303996Z","iopub.status.idle":"2024-10-28T08:28:46.365270Z","shell.execute_reply":"2024-10-28T08:28:46.364479Z","shell.execute_reply.started":"2024-10-28T08:28:46.304292Z"},"trusted":true},"outputs":[],"source":["df_answer = df_answer[df_answer[\"CorrectAnswer\"] != df_answer[\"Option\"]]\n","df_answer[\"Prompt\"] = df_answer.apply(\n","    lambda row: apply_template(row, tokenizer), axis=1\n",")\n","df_answer.to_parquet(\"test.parquet\", index=False)\n","\n","df = pd.read_parquet(\"test.parquet\")"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-10-28T08:28:46.371186Z","iopub.status.busy":"2024-10-28T08:28:46.370634Z","iopub.status.idle":"2024-10-28T08:28:52.147015Z","shell.execute_reply":"2024-10-28T08:28:52.146012Z","shell.execute_reply.started":"2024-10-28T08:28:46.371150Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Users\\gamem\\mambaforge\\envs\\eedi\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n","  warnings.warn(\n","c:\\Users\\gamem\\mambaforge\\envs\\eedi\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n","  warnings.warn(\n","c:\\Users\\gamem\\mambaforge\\envs\\eedi\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","  warnings.warn(\n","c:\\Users\\gamem\\mambaforge\\envs\\eedi\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n","  warnings.warn(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9cb63bf00e9046d9a7eb6984b9afbc10","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["generator = pipeline(\n","    \"text-generation\",\n","    model=model_id,\n","    model_kwargs={\"temperature\": 0.7, \"top_p\": 0.9},\n","    device=\"cuda\",\n",")"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-10-28T08:28:52.148648Z","iopub.status.busy":"2024-10-28T08:28:52.148318Z","iopub.status.idle":"2024-10-28T08:28:52.153103Z","shell.execute_reply":"2024-10-28T08:28:52.151822Z","shell.execute_reply.started":"2024-10-28T08:28:52.148613Z"},"trusted":true},"outputs":[],"source":["# df = df.head(1)"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-10-28T08:28:52.154440Z","iopub.status.busy":"2024-10-28T08:28:52.154133Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["  0%|          | 0/9 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"," 11%|█         | 1/9 [00:42<05:42, 42.80s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"," 22%|██▏       | 2/9 [01:24<04:54, 42.03s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"," 33%|███▎      | 3/9 [02:05<04:08, 41.44s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"," 44%|████▍     | 4/9 [03:46<05:25, 65.10s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"," 56%|█████▌    | 5/9 [04:35<03:56, 59.15s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"," 67%|██████▋   | 6/9 [06:29<03:53, 77.97s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"," 78%|███████▊  | 7/9 [13:21<06:14, 187.32s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"," 89%|████████▉ | 8/9 [15:07<02:41, 161.16s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","100%|██████████| 9/9 [16:58<00:00, 113.13s/it]\n"]}],"source":["responses = []\n","for v in tqdm(df[\"Prompt\"].values):\n","    out = generator(v, max_new_tokens=4096)\n","    responses.append(out)\n","\n","responses = [x[0][\"generated_text\"] for x in responses]\n","df[\"FullResponse\"] = responses"]},{"cell_type":"code","execution_count":16,"metadata":{"trusted":true},"outputs":[],"source":["# df"]},{"cell_type":"code","execution_count":17,"metadata":{"trusted":true},"outputs":[],"source":["def remove_prompt(record):\n","    l = len(record[\"Prompt\"])\n","    value = record[\"FullResponse\"][l:]\n","    return value\n","\n","df[\"FullResponse\"] = df.apply(remove_prompt, axis=1)"]},{"cell_type":"code","execution_count":18,"metadata":{"trusted":true},"outputs":[],"source":["# df"]},{"cell_type":"code","execution_count":19,"metadata":{"trusted":true},"outputs":[],"source":["def extract_response(text):\n","    subresponses = re.findall(r\"<response>(?s:.*?)</response>\", text)\n","    subresponses = [x.strip().replace(\"<response>\", \"\").replace(\"</response>\", \"\") for x in subresponses]\n","    return \" \".join(subresponses).strip()\n","\n","responses = [extract_response(x) for x in df[\"FullResponse\"]]\n","df[\"Misconception\"] = responses\n","df.to_parquet(\"output.parquet\", index=False)"]},{"cell_type":"code","execution_count":20,"metadata":{"trusted":true},"outputs":[],"source":["## get semantically similar misconceptions"]},{"cell_type":"code","execution_count":21,"metadata":{"trusted":true},"outputs":[],"source":["# df = pd.read_parquet(\"output.parquet\")"]},{"cell_type":"code","execution_count":22,"metadata":{"trusted":true},"outputs":[],"source":["# df.head(1).transpose()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":23,"metadata":{"trusted":true},"outputs":[],"source":["# !pip install -U sentence-transformers"]},{"cell_type":"code","execution_count":24,"metadata":{"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e3c1ec2bc7bb4ccebc1d0e4b053f3973","version_major":2,"version_minor":0},"text/plain":["modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"73c5441e7dd8456bafd01af2e50b5375","version_major":2,"version_minor":0},"text/plain":["config_sentence_transformers.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"aa22942c4af3443595f8384b363ed322","version_major":2,"version_minor":0},"text/plain":["README.md:   0%|          | 0.00/94.8k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"00c0ff44434d46299f416e0608aa74a3","version_major":2,"version_minor":0},"text/plain":["sentence_bert_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f29a5487aba441df8108d8848e1e81cd","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/743 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3c53da350fb44e43b5b97c91188dc126","version_major":2,"version_minor":0},"text/plain":["model.safetensors:   0%|          | 0.00/133M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9dd7d87dd6a54882999e99f5b786af5b","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/366 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4c9ce51e736e47a581fc1c831ea379a8","version_major":2,"version_minor":0},"text/plain":["vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"bb44361664694078abe17c668a3e03a9","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9b712d0419e14b8bb4256547c94149cd","version_major":2,"version_minor":0},"text/plain":["special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"deef681b5ffa48b2a09b7bb53948e60c","version_major":2,"version_minor":0},"text/plain":["1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["import pandas as pd\n","from sentence_transformers import SentenceTransformer, util\n","from sklearn.metrics.pairwise import cosine_similarity\n","from scipy.spatial.distance import cdist\n","import numpy as np\n","\n","df = pd.read_parquet(\"output.parquet\")\n","df_misconception_mapping = pd.read_csv(\"data/misconception_mapping.csv\")\n","\n","# model = SentenceTransformer('/kaggle/input/bge-large-en-v1-5')\n","model = SentenceTransformer('BAAI/bge-small-en-v1.5')\n","# model = SentenceTransformer('/kaggle/input/bge-small-en-v1.5/transformers/bge/2')\n","# PREFIX = \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n\"\n","# input_features = df[\"Misconception\"].str.lstrip(PREFIX).str.split(\"\\n\\nYour task:\").str[0]\n","# input_features = df[\"Misconception\"]\n","\n","\n","\n","# embedding_query = model.encode(input_features+ \"\\n----\\n\" + df[\"fullLLMText\"]) # , convert_to_tensor=True\n","# embedding_Misconception = model.encode(df_misconception_mapping.MisconceptionName.values)\n","\n","# # compute cosine and euclid distance\n","# # Compute similarities\n","# cosine_similarities = cosine_similarity(embedding_query, embedding_Misconception)\n","# # Euclidean distance\n","# euclidean_distances = cdist(embedding_query, embedding_Misconception, metric='euclidean')\n","# euclidean_similarities = 1 / (1 + euclidean_distances)  # Convert distance to similarity\n","# # Combination of cosine and euclidean\n","# combined_similarities = (cosine_similarities + euclidean_similarities) / 2\n","# # Use the combined_similarities for sorting\n","# test_sorted_indices = np.argsort(-combined_similarities, axis=1)\n","\n","# # top25ids = util.semantic_search(embedding_query, embedding_Misconception, top_k=25)\n","# df[\"MisconceptionId\"] = test_sorted_indices[:, :25].tolist()\n","# df[\"MisconceptionId\"] = df[\"MisconceptionId\"].apply(lambda x: ' '.join(map(str, x)))\n","# df[[\"QuestionId_Answer\", \"MisconceptionId\"]].to_csv(\"submission.csv\", index=False)"]},{"cell_type":"code","execution_count":25,"metadata":{"trusted":true},"outputs":[],"source":["embedding_query = model.encode(df[\"Misconception\"])"]},{"cell_type":"code","execution_count":26,"metadata":{"trusted":true},"outputs":[],"source":["embedding_miscon = model.encode(df_misconception_mapping.MisconceptionName.values)"]},{"cell_type":"code","execution_count":27,"metadata":{"trusted":true},"outputs":[],"source":["cosine_similarities = cosine_similarity(embedding_query, embedding_miscon)"]},{"cell_type":"code","execution_count":28,"metadata":{"trusted":true},"outputs":[],"source":["rev_sorted_indices = np.argsort(-cosine_similarities, axis=1)"]},{"cell_type":"code","execution_count":29,"metadata":{"trusted":true},"outputs":[],"source":["# rev_sorted_indices[:, :25]"]},{"cell_type":"code","execution_count":30,"metadata":{"trusted":true},"outputs":[],"source":["# df.head(1).transpose()"]},{"cell_type":"code","execution_count":31,"metadata":{"trusted":true},"outputs":[],"source":["df[\"MisconceptionId\"] = rev_sorted_indices[:, :25].tolist()"]},{"cell_type":"code","execution_count":32,"metadata":{"trusted":true},"outputs":[],"source":["df[\"MisconceptionId\"] = df[\"MisconceptionId\"].apply(lambda x: ' '.join(map(str, x)))"]},{"cell_type":"code","execution_count":33,"metadata":{"trusted":true},"outputs":[],"source":["df[\"QuestionId_Answer\"] = df[\"QuestionId\"].astype(str) + \"_\" + df[\"CorrectAnswer\"]"]},{"cell_type":"code","execution_count":34,"metadata":{"trusted":true},"outputs":[],"source":["# df.head(1).transpose()"]},{"cell_type":"code","execution_count":35,"metadata":{"trusted":true},"outputs":[],"source":["df[[\"QuestionId_Answer\", \"MisconceptionId\"]].to_csv(\"submission.csv\", index=False)"]},{"cell_type":"code","execution_count":36,"metadata":{"trusted":true},"outputs":[],"source":["# def apk(actual, predicted, k=25):\n","#     \"\"\"\n","#     Computes the average precision at k.\n","\n","#     This function computes the average prescision at k between two lists of\n","#     items.\n","\n","#     Parameters\n","#     ----------\n","#     actual : list\n","#              A list of elements that are to be predicted (order doesn't matter)\n","#     predicted : list\n","#                 A list of predicted elements (order does matter)\n","#     k : int, optional\n","#         The maximum number of predicted elements\n","\n","#     Returns\n","#     -------\n","#     score : double\n","#             The average precision at k over the input lists\n","#     \"\"\"\n","\n","#     if not actual:\n","#         return 0.0\n","\n","#     if len(predicted)>k:\n","#         predicted = predicted[:k]\n","\n","#     score = 0.0\n","#     num_hits = 0.0\n","\n","#     for i,p in enumerate(predicted):\n","#         # first condition checks whether it is valid prediction\n","#         # second condition checks if prediction is not repeated\n","#         if p in actual and p not in predicted[:i]:\n","#             num_hits += 1.0\n","#             score += num_hits / (i+1.0)\n","\n","#     return score / min(len(actual), k)\n","\n","# def mapk(actual, predicted, k=25):\n","#     \"\"\"\n","#     Computes the mean average precision at k.\n","\n","#     This function computes the mean average prescision at k between two lists\n","#     of lists of items.\n","\n","#     Parameters\n","#     ----------\n","#     actual : list\n","#              A list of lists of elements that are to be predicted\n","#              (order doesn't matter in the lists)\n","#     predicted : list\n","#                 A list of lists of predicted elements\n","#                 (order matters in the lists)\n","#     k : int, optional\n","#         The maximum number of predicted elements\n","\n","#     Returns\n","#     -------\n","#     score : double\n","#             The mean average precision at k over the input lists\n","#     \"\"\"\n","\n","#     return np.mean([apk(a,p,k) for a,p in zip(actual, predicted)])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"databundleVersionId":9738540,"sourceId":82695,"sourceType":"competition"},{"isSourceIdPinned":true,"modelId":121027,"modelInstanceId":100933,"sourceId":120002,"sourceType":"modelInstanceVersion"},{"isSourceIdPinned":true,"modelId":121027,"modelInstanceId":100936,"sourceId":120005,"sourceType":"modelInstanceVersion"},{"isSourceIdPinned":true,"modelId":33601,"modelInstanceId":23286,"sourceId":27644,"sourceType":"modelInstanceVersion"}],"dockerImageVersionId":30787,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"eedi","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.15"}},"nbformat":4,"nbformat_minor":4}
