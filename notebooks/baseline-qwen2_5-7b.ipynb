{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import shutil\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "from uuid import uuid4\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from IPython.display import display\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from torch import Tensor\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers.tokenization_utils_base import BatchEncoding\n",
    "from transformers.tokenization_utils_fast import PreTrainedTokenizerFast\n",
    "from transformers.models.qwen2.tokenization_qwen2_fast import Qwen2TokenizerFast\n",
    "from transformers.models.qwen2.modeling_qwen2 import Qwen2ForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"DEVICE:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eedi_train_csv = \"data/train_data.csv\"\n",
    "eedi_test_csv = \"data/test_data.csv\"\n",
    "eedi_miscon_csv = \"data/misconception_mapping.csv\"\n",
    "llm_model_id = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "sbert_model_id = \"/home/e/e1374073/models/bge-large-en-finetune-v1\"\n",
    "# sbert_model_id = \"/root/projects/eedi/.tmp/bge-large-en-finetune-v1\" # TODO : remove when slurm\n",
    "submission_csv = \"submission.csv\"\n",
    "intermediate_dir = \".intm\"\n",
    "last_dir = \".last\"\n",
    "random_seed = 20241030\n",
    "sample_size = -1  # -1 to run all data # TODO reset if slurm\n",
    "batch_size = 20\n",
    "disable_tqdm = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"Question: {Question}\n",
    "Incorrect Answer: {IncorrectAnswer}\n",
    "Correct Answer: {CorrectAnswer}\n",
    "Construct Name: {ConstructName}\n",
    "Subject Name: {SubjectName}\n",
    "\n",
    "Your task: Identify the misconception behind Incorrect Answer. Answer concisely and generically inside <response></response>.\n",
    "Before answering the question think step by step concisely in 1-2 sentence inside <thinking></thinking> tag and respond your final misconception inside <response></response> tag.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_template(row, tokenizer):\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": prompt.format(\n",
    "                ConstructName=row[\"ConstructName\"],\n",
    "                SubjectName=row[\"SubjectName\"],\n",
    "                Question=row[\"QuestionText\"],\n",
    "                IncorrectAnswer=row[f\"CorrectAnswerText\"],\n",
    "                CorrectAnswer=row[f\"AnswerText\"],\n",
    "            ),\n",
    "        }\n",
    "    ]\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "    )\n",
    "    return text\n",
    "\n",
    "\n",
    "def get_correct_answer(row):\n",
    "    if row[\"CorrectAnswer\"] == \"A\":\n",
    "        return row[\"AnswerAText\"]\n",
    "    elif row[\"CorrectAnswer\"] == \"B\":\n",
    "        return row[\"AnswerBText\"]\n",
    "    elif row[\"CorrectAnswer\"] == \"C\":\n",
    "        return row[\"AnswerCText\"]\n",
    "    elif row[\"CorrectAnswer\"] == \"D\":\n",
    "        return row[\"AnswerDText\"]\n",
    "    return None\n",
    "\n",
    "\n",
    "def process_option(x, regex):\n",
    "    result = re.search(regex, x)\n",
    "    return str(result.group(1)) if result else \"\"\n",
    "\n",
    "\n",
    "def remove_prompt(record):\n",
    "    l = record[\"FullResponse\"].index(\"<|im_start|>assistant\") + 21\n",
    "    value = record[\"FullResponse\"][l:].strip()\n",
    "    if value == \"\":\n",
    "        value = \"No Misconception Found\"\n",
    "    return value\n",
    "\n",
    "\n",
    "def extract_response(text):\n",
    "    subresponses = re.findall(r\"<response>(?s:.*?)</response>\", text)\n",
    "    subresponses = [x.strip().replace(\"<response>\", \"\").replace(\"</response>\", \"\") for x in subresponses]\n",
    "    return \" \".join(subresponses).strip()\n",
    "\n",
    "\n",
    "def dfpeek(title: str, df: pd.DataFrame) -> None:\n",
    "    print(\">>>>>>>>>>\", title, \">>>>>>>>>\")\n",
    "    display(df.head(1).transpose())\n",
    "    print(\"<<<<<<<<<<\", title, \"<<<<<<<<<<\", end=\"\\n\\n\")\n",
    "\n",
    "\n",
    "def dfpersist(trigger: bool, df: pd.DataFrame, int_dir: str, run_id: str, fn: str) -> None:\n",
    "    if not trigger:\n",
    "        return\n",
    "    assert run_id is not None\n",
    "    d = Path(int_dir) / run_id\n",
    "    d_last = Path(int_dir) / last_dir\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "    p = d / fn\n",
    "    if p.exists():\n",
    "        raise FileExistsError(p.as_posix())\n",
    "    d_last.mkdir(parents=True, exist_ok=True)\n",
    "    p_last = d_last / fn\n",
    "    df.to_parquet(p, index=False)\n",
    "    shutil.copyfile(p, p_last)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_base_data(*, persist: bool = False, run_id: str = None) -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    # read info\n",
    "    df = pd.read_csv(\n",
    "        eedi_test_csv,\n",
    "        dtype={\n",
    "            \"MisconceptionAId\": \"Int64\",\n",
    "            \"MisconceptionBId\": \"Int64\",\n",
    "            \"MisconceptionCId\": \"Int64\",\n",
    "            \"MisconceptionDId\": \"Int64\",\n",
    "        },\n",
    "    ).fillna(-1)\n",
    "    df_miscon = pd.read_csv(\n",
    "        eedi_miscon_csv,\n",
    "        dtype={\n",
    "            \"MisconceptionId\": \"Int64\",\n",
    "        },\n",
    "    )\n",
    "\n",
    "    # store correct answer\n",
    "    df[\"CorrectAnswerText\"] = df.apply(get_correct_answer, axis=1)\n",
    "\n",
    "    # pivot out each wrong answer into its own row, currently the 3 wrong answers are within the same record\n",
    "    df_x = df.melt(\n",
    "        id_vars=[\n",
    "            \"QuestionId\",\n",
    "            \"ConstructName\",\n",
    "            \"SubjectName\",\n",
    "            \"QuestionText\",\n",
    "            \"CorrectAnswer\",\n",
    "            \"CorrectAnswerText\",\n",
    "        ],\n",
    "        value_vars=[\n",
    "            \"AnswerAText\",\n",
    "            \"AnswerBText\",\n",
    "            \"AnswerCText\",\n",
    "            \"AnswerDText\",\n",
    "        ],\n",
    "        var_name=\"Option\",\n",
    "        value_name=\"AnswerText\",\n",
    "    )\n",
    "    df_y = df.melt(\n",
    "        id_vars=[\n",
    "            \"QuestionId\",\n",
    "        ],\n",
    "        value_vars=[\n",
    "            \"MisconceptionAId\",\n",
    "            \"MisconceptionBId\",\n",
    "            \"MisconceptionCId\",\n",
    "            \"MisconceptionDId\",\n",
    "        ],\n",
    "        var_name=\"Option\",\n",
    "        value_name=\"MisconceptionId\",\n",
    "    )\n",
    "\n",
    "    # remap option values of from \"xxxxXxxxx\" to \"X\"\n",
    "    df_x[\"Option\"] = df_x[\"Option\"].map(partial(process_option, regex=r\"Answer([A-D])Text\"))\n",
    "    df_y[\"Option\"] = df_y[\"Option\"].map(partial(process_option, regex=r\"Misconception([A-D])Id\"))\n",
    "\n",
    "    # mark correct answers\n",
    "    df_x[\"IsCorrectAnswer\"] = df_x[\"CorrectAnswer\"] == df_x[\"Option\"]\n",
    "\n",
    "    # create primary key, drop components, reorder col\n",
    "    df_x[\"QuestionId_Answer\"] = df_x[\"QuestionId\"].astype(str) + \"_\" + df_x[\"Option\"].astype(str)\n",
    "    df_y[\"QuestionId_Answer\"] = df_y[\"QuestionId\"].astype(str) + \"_\" + df_y[\"Option\"].astype(str)\n",
    "    df_x.drop(columns=[\"QuestionId\", \"Option\"], inplace=True)\n",
    "    df_y.drop(columns=[\"QuestionId\", \"Option\"], inplace=True)\n",
    "    df_x = df_x[[\"QuestionId_Answer\"] + [c for c in df_x.columns if c != \"QuestionId_Answer\"]]\n",
    "    df_y = df_y[[\"QuestionId_Answer\"] + [c for c in df_y.columns if c != \"QuestionId_Answer\"]]\n",
    "\n",
    "    # map misconception text to labels\n",
    "    df_y = df_y.join(df_miscon, on=\"MisconceptionId\", how=\"left\", lsuffix=\"a\", rsuffix=\"b\")\n",
    "    df_y = df_y[[\"QuestionId_Answer\", \"MisconceptionId\", \"MisconceptionName\"]]\n",
    "\n",
    "    # merge datasets\n",
    "    df_xy = df_x.merge(df_y, how=\"left\", on=\"QuestionId_Answer\")\n",
    "\n",
    "    # persist df_xy\n",
    "    dfpersist(persist, df_xy, intermediate_dir, run_id, \"df_xy.parquet\")\n",
    "\n",
    "    return df_xy, df_miscon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_by_wrong_answers_only(df_xy: pd.DataFrame) -> pd.DataFrame:\n",
    "    df_xy = df_xy[~df_xy[\"IsCorrectAnswer\"]]\n",
    "    return df_xy\n",
    "\n",
    "\n",
    "def filter_by_wrong_answers_and_misconceptions(df_xy: pd.DataFrame) -> pd.DataFrame:\n",
    "    return df_xy[(~df_xy[\"IsCorrectAnswer\"]) & (df_xy[\"MisconceptionId\"] != -1)]\n",
    "\n",
    "\n",
    "def no_filter(df_xy: pd.DataFrame) -> pd.DataFrame:\n",
    "    return df_xy\n",
    "\n",
    "\n",
    "def filter_data(df_xy: pd.DataFrame, persist: bool = False, run_id: str = None) -> pd.DataFrame:\n",
    "    filter_func = filter_by_wrong_answers_and_misconceptions\n",
    "    df_xy_filtered = filter_func(df_xy)\n",
    "    dfpersist(persist, df_xy_filtered, intermediate_dir, run_id, \"df_xy_filtered.parquet\")\n",
    "    return df_xy_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_for_llm(\n",
    "    tokenizer: Qwen2TokenizerFast,\n",
    "    df_xy: pd.DataFrame,\n",
    "    *,\n",
    "    persist: bool = False,\n",
    "    persist_fn: str = \"df_prompt.parquet\",\n",
    "    run_id: str = None,\n",
    ") -> tuple[pd.DataFrame, list[BatchEncoding]]:\n",
    "    df_prompt = df_xy.copy(deep=True)\n",
    "    df_prompt[\"Prompt\"] = df_prompt.apply(\n",
    "        partial(apply_template, tokenizer=tokenizer),\n",
    "        axis=1,\n",
    "    )\n",
    "    df_prompt = df_prompt[[\"QuestionId_Answer\", \"Prompt\"]]\n",
    "    dfpersist(persist, df_prompt, intermediate_dir, run_id, persist_fn)\n",
    "    prompts = df_prompt[\"Prompt\"].to_list()\n",
    "    prompt_batches = [prompts[i : i + batch_size] for i in range(0, len(prompts), batch_size)]\n",
    "    model_inputs_batches = []\n",
    "    for pb in prompt_batches:\n",
    "        model_inputs = tokenizer(pb, return_tensors=\"pt\", padding=True).to(device)\n",
    "        model_inputs_batches.append(model_inputs)\n",
    "    return df_prompt, model_inputs_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_zeroshot(\n",
    "    model: Qwen2ForCausalLM,\n",
    "    tokenizer: PreTrainedTokenizerFast,\n",
    "    token_batches: list[BatchEncoding],\n",
    "    df_prompt,\n",
    "    *,\n",
    "    persist: bool = False,\n",
    "    persist_fn: str = \"df_responses.parquet\",\n",
    "    run_id: str = None,\n",
    ") -> pd.DataFrame:\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output_ids_batches: list[Tensor] = []\n",
    "        for tokens in tqdm(token_batches, disable=disable_tqdm):\n",
    "            output_ids_batch: Tensor = model.generate(\n",
    "                tokens.input_ids,\n",
    "                max_new_tokens=4096,\n",
    "                num_return_sequences=1,\n",
    "                attention_mask=tokens.attention_mask,\n",
    "            )\n",
    "            output_ids_batches.append(output_ids_batch.cpu())\n",
    "    responses = []\n",
    "    for output_ids_batch in output_ids_batches:\n",
    "        responses.extend(tokenizer.batch_decode(output_ids_batch))\n",
    "    df_prompt[\"FullResponse\"] = responses\n",
    "    df_prompt[\"Response\"] = df_prompt.apply(remove_prompt, axis=1)\n",
    "    df_prompt[\"Misconception\"] = [extract_response(x) for x in df_prompt[\"Response\"]]\n",
    "    dfpersist(persist, df_prompt, intermediate_dir, run_id, persist_fn)\n",
    "    return df_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_misconceptions(\n",
    "    model: SentenceTransformer,\n",
    "    df_responses: pd.DataFrame,\n",
    "    df_miscon: pd.DataFrame,\n",
    "    *,\n",
    "    persist: bool = False,\n",
    "    persist_fn: str = \"df_submission.parquet\",\n",
    "    run_id: str = None,\n",
    "):\n",
    "    print(\">> generate_misconceptions\")\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        embedding_query = model.encode(df_responses[\"Misconception\"].values)\n",
    "        embedding_miscon = model.encode(df_miscon[\"MisconceptionName\"].values)\n",
    "    cosine_similarities = cosine_similarity(embedding_query, embedding_miscon)\n",
    "    rev_sorted_indices = np.argsort(-cosine_similarities, axis=1)\n",
    "    df_responses[\"MisconceptionId\"] = rev_sorted_indices[:, :25].tolist()\n",
    "    df_responses[\"MisconceptionId\"] = df_responses[\"MisconceptionId\"].apply(lambda x: \" \".join(map(str, x)))\n",
    "    df_submission = df_responses[[\"QuestionId_Answer\", \"MisconceptionId\"]]\n",
    "    dfpersist(persist, df_submission, intermediate_dir, run_id, persist_fn)\n",
    "    return df_submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apk(actual, predicted, k=25):\n",
    "    if not actual:\n",
    "        return 0.0\n",
    "\n",
    "    actual = [actual]\n",
    "    predicted = list(map(int, predicted.split()))\n",
    "\n",
    "    if len(predicted) > k:\n",
    "        predicted = predicted[:k]\n",
    "\n",
    "    score = 0.0\n",
    "    num_hits = 0.0\n",
    "\n",
    "    for i, p in enumerate(predicted):\n",
    "        if p in actual and p not in predicted[:i]:\n",
    "            num_hits += 1.0\n",
    "            score += num_hits / (i + 1.0)\n",
    "\n",
    "    return score / min(len(actual), k)\n",
    "\n",
    "\n",
    "def mapk(actual, predicted, k=25):\n",
    "    return np.mean([apk(a, p, k) for a, p in zip(actual, predicted)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(df_xy, df_submission, *, run_id: str, fn: str = \"results.txt\"):\n",
    "    results = mapk(df_xy[\"MisconceptionId\"].to_list(), df_submission[\"MisconceptionId\"].to_list())\n",
    "    assert run_id is not None\n",
    "    d = Path(intermediate_dir) / run_id\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "    p: Path = d / fn\n",
    "    if p.exists():\n",
    "        raise FileExistsError(p.as_posix())\n",
    "    p.write_text(f\"Results of MapK=25 : {results}\")\n",
    "    print(\"Results of MapK=25 :\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main() -> None:\n",
    "    print(\"name: baseline qwen2_5-7b\")\n",
    "    run_id = str(uuid4())\n",
    "    print(\"run_id:\", run_id)\n",
    "    df_xy, df_miscon = prepare_base_data(persist=True, run_id=run_id)\n",
    "    df_xy = filter_data(df_xy, persist=True, run_id=run_id)\n",
    "    if sample_size > 0:\n",
    "        df_xy = df_xy.sample(sample_size)\n",
    "    tokenizer: Qwen2TokenizerFast = AutoTokenizer.from_pretrained(llm_model_id, padding_side=\"left\")\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    model: Qwen2ForCausalLM = AutoModelForCausalLM.from_pretrained(\n",
    "        llm_model_id,\n",
    "        torch_dtype=\"auto\",\n",
    "        device_map=\"auto\",\n",
    "    )\n",
    "    model.generation_config.pad_token_id = tokenizer.pad_token_id\n",
    "    sbert_model = SentenceTransformer(sbert_model_id)\n",
    "    df_prompt, token_batches = tokenize_for_llm(tokenizer, df_xy, persist=True, run_id=run_id)\n",
    "    df_responses = generate_zeroshot(model, tokenizer, token_batches, df_prompt, persist=True, run_id=run_id)\n",
    "    df_submission = generate_misconceptions(sbert_model, df_responses, df_miscon, persist=True, run_id=run_id)\n",
    "    evaluate(df_xy, df_submission, run_id=run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
